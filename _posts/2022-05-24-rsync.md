---
layout: post
title: rsync is slow and hard
categories: [Random]
---

`rsync` is incredible, and you should use it! But, 95% of the time what I need to do with a utility like rsync is to back
up my `Downloads` folder to the large HDD on my desktop before I distro-hop (yes, I'm that person). Everything else
is backed up to github anyway, because I basically do nothing but code on my computers!

Using `rsync` (or really `rsync -azP`) for this works okay, but I run into another problem: I intentionally bought a
pretty slow laptop because it gets better battery life. This is not really an issue unless I am compiling llvm or
trying to compress a 200GB `Downloads` folder. There's a compounding issue in that I frequently am doing this
over the summer, which means that since I am in school at the moment, I am doing this in an airbnb typically
owned by a less than tech savvy landlord and the Wifi is not...great.

So what do we do? We don't want to compress over the wire, we *definitely* don't want to write our own utility...
but FTP is fast! And also...it's pretty annoying to use. `put -r` of `sftp` fame doesn't exist on regular old
`ftp` and besides, setting up a server and writing a unit file will take an hour.

What I do is this:

On the server:

```sh
$ python3 -m pip install pyftpdlib
$ python3 -m pyftpdlib --directory=. --port [PORT] --write
```

On the client, I install [`lftp`](https://github.com/lavv17/lftp), the best FTP client I am aware of, and use its
"reverse mirror" functionality to recursively copy over my full directory:

```sh
$ lftp -u anonymous,password -p [PORT] [HOSTNAME]
lftp~> mirror -R Downloads
```

That's it! Wait an hour or two, and you are ready to distrohop to your heart's content.
